{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy \n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import collections\n",
    "import pymorphy3\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "m = Mystem()\n",
    "nlp = spacy.load('ru_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtok = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "\n",
    "with open ('1886 Скучная история .txt', 'r', encoding='windows-1251') as book1:\n",
    "    text1 = book1.read()\n",
    "words1 = wordtok.tokenize(text1)\n",
    "words1ful = word_tokenize(text1)\n",
    "with open ('1889 Лошадиная фамилия.txt', 'r', encoding='windows-1251') as book2:\n",
    "    text2 = book2.read()\n",
    "words2 = wordtok.tokenize(text2)\n",
    "words2ful = word_tokenize(text2)\n",
    "with open ('1896 Дом с мезонином.txt', 'r', encoding='windows-1251') as book3:\n",
    "    text3 = book3.read()\n",
    "words3 = wordtok.tokenize(text3)\n",
    "words3ful = word_tokenize(text3)\n",
    "with open ('1899 Дама с собачкой.txt', 'r', encoding='windows-1251') as book4:\n",
    "    text4 = book4.read()\n",
    "words4 = wordtok.tokenize(text4)\n",
    "words4ful = word_tokenize(text4)\n",
    "with open ('1900 Три сестры.txt', 'r', encoding='windows-1251') as book5:\n",
    "    text5 = book5.read()\n",
    "words5 = wordtok.tokenize(text5)\n",
    "words5ful = word_tokenize(text5)\n",
    "with open ('1904 Вишневый сад.txt', 'r', encoding='windows-1251') as book6:\n",
    "    text6 = book6.read()\n",
    "words6 = wordtok.tokenize(text6)\n",
    "words6ful = word_tokenize(text6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('russian')\n",
    "words1_filt = [word for word in words1 if word not in stop]\n",
    "words2_filt = [word for word in words2 if word not in stop]\n",
    "words3_filt = [word for word in words3 if word not in stop]\n",
    "words4_filt = [word for word in words4 if word not in stop]\n",
    "words5_filt = [word for word in words5 if word not in stop]\n",
    "words6_filt = [word for word in words6 if word not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemmatized_words1 = m.lemmatize(' '.join(words1_filt))\n",
    "lemmatized_words2 = m.lemmatize(' '.join(words2_filt))\n",
    "lemmatized_words3 = m.lemmatize(' '.join(words3_filt))\n",
    "lemmatized_words4 = m.lemmatize(' '.join(words4_filt))\n",
    "lemmatized_words5 = m.lemmatize(' '.join(words5_filt))\n",
    "lemmatized_words6 = m.lemmatize(' '.join(words6_filt))\n",
    "\n",
    "\n",
    "words1_filt_n = [word for word in lemmatized_words1 if word not in stop]\n",
    "words2_filt_n = [word for word in lemmatized_words2 if word not in stop]\n",
    "words3_filt_n = [word for word in lemmatized_words3 if word not in stop]\n",
    "words4_filt_n = [word for word in lemmatized_words4 if word not in stop]\n",
    "words5_filt_n = [word for word in lemmatized_words5 if word not in stop]\n",
    "words6_filt_n = [word for word in lemmatized_words6 if word not in stop]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Часто встречающиеся слова***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1886 Скучная история\n",
      " : 11411\n",
      "говорить: 123\n",
      "который: 104\n",
      "это: 99\n",
      "свой: 91\n",
      "человек: 76\n",
      "катя: 60\n",
      "знать: 58\n",
      "мочь: 57\n",
      "год: 44\n",
      "\n",
      "1889 Лошадиная фамилия\n",
      " : 644\n",
      "фамилия: 15\n",
      "генерал: 12\n",
      "зуб: 9\n",
      "иван: 9\n",
      "евсеич: 9\n",
      "ваш: 9\n",
      "превосходительство: 9\n",
      "васильич: 8\n",
      "лошадиный: 7\n",
      "\n",
      "1896 Дом с мезонином\n",
      " : 3572\n",
      "говорить: 36\n",
      "который: 34\n",
      "лида: 32\n",
      "сказать: 31\n",
      "человек: 30\n",
      "свой: 30\n",
      "жизнь: 23\n",
      "женя: 22\n",
      "весь: 21\n",
      "\n",
      "1899 Дама с собачкой\n",
      " : 3009\n",
      "гуров: 35\n",
      "говорить: 34\n",
      "анна: 27\n",
      "который: 26\n",
      "сергеевна: 26\n",
      "это: 24\n",
      "жизнь: 23\n",
      "женщина: 20\n",
      "человек: 18\n",
      "\n",
      "1900 Три сестры\n",
      " : 11252\n",
      "ирина: 182\n",
      "маша: 162\n",
      "ольга: 113\n",
      "уходить: 108\n",
      "это: 102\n",
      "андрей: 101\n",
      "говорить: 97\n",
      "вершинин: 90\n",
      "чебутыкин: 90\n",
      "\n",
      "1904 Вишневый сад\n",
      " : 9185\n",
      "любовь: 152\n",
      "андреевна: 145\n",
      "лопахин: 124\n",
      "варя: 118\n",
      "аня: 110\n",
      "гаев: 88\n",
      "трофимов: 85\n",
      "уходить: 78\n",
      "это: 76\n"
     ]
    }
   ],
   "source": [
    "name_book = ['1886 Скучная история', '1889 Лошадиная фамилия', '1896 Дом с мезонином', '1899 Дама с собачкой', '1900 Три сестры', '1904 Вишневый сад']\n",
    "word_frequencies = collections.Counter(words1_filt_n)\n",
    "\n",
    "most_common_words = word_frequencies.most_common(10)\n",
    "print(name_book[0])\n",
    "for word, frequency in most_common_words:\n",
    "    print(f'{word}: {frequency}')\n",
    "print(\"\")\n",
    "\n",
    "word_frequencies = collections.Counter(words2_filt_n)\n",
    "\n",
    "most_common_words = word_frequencies.most_common(10)\n",
    "print(name_book[1])\n",
    "for word, frequency in most_common_words:\n",
    "    print(f'{word}: {frequency}')\n",
    "print(\"\")\n",
    "\n",
    "word_frequencies = collections.Counter(words3_filt_n)\n",
    "\n",
    "most_common_words = word_frequencies.most_common(10)\n",
    "print(name_book[2])\n",
    "for word, frequency in most_common_words:\n",
    "    print(f'{word}: {frequency}')\n",
    "print(\"\")\n",
    "\n",
    "word_frequencies = collections.Counter(words4_filt_n)\n",
    "\n",
    "most_common_words = word_frequencies.most_common(10)\n",
    "print(name_book[3])\n",
    "for word, frequency in most_common_words:\n",
    "    print(f'{word}: {frequency}')\n",
    "print(\"\")\n",
    "\n",
    "word_frequencies = collections.Counter(words5_filt_n)\n",
    "\n",
    "most_common_words = word_frequencies.most_common(10)\n",
    "print(name_book[4])\n",
    "for word, frequency in most_common_words:\n",
    "    print(f'{word}: {frequency}')\n",
    "print(\"\")\n",
    "\n",
    "word_frequencies = collections.Counter(words6_filt_n)\n",
    "\n",
    "most_common_words = word_frequencies.most_common(10)\n",
    "print(name_book[5])\n",
    "for word, frequency in most_common_words:\n",
    "    print(f'{word}: {frequency}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Как мне кажется это ничего не показывает, но вывод такой, что самое частое слово это \"говорить\"***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Далее рассмотрю среднее количество слов в предложениях***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя длина предложений в 1886 Скучная история: 17.93\n",
      "Средняя длина предложений в 1889 Лошадиная фамилия: 9.96\n",
      "Средняя длина предложений в 1896 Дом с мезонином: 24.67\n",
      "Средняя длина предложений в 1899 Дама с собачкой: 23.02\n",
      "Средняя длина предложений в 1900 Три сестры: 7.46\n",
      "Средняя длина предложений в 1904 Вишневый сад: 7.41\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc1 = nlp(text1)\n",
    "doc2 = nlp(text2)\n",
    "doc3 = nlp(text3)\n",
    "doc4 = nlp(text4)\n",
    "doc5 = nlp(text5)\n",
    "doc6 = nlp(text6)\n",
    "\n",
    "sentence_lengths1 = []\n",
    "sentence_lengths2 = []\n",
    "sentence_lengths3 = []\n",
    "sentence_lengths4 = []\n",
    "sentence_lengths5 = []\n",
    "sentence_lengths6 = []\n",
    "\n",
    "for sentence in doc1.sents:\n",
    "    sentence_length = len(sentence)\n",
    "    sentence_lengths1.append(sentence_length)\n",
    "for sentence in doc2.sents:\n",
    "    sentence_length = len(sentence)\n",
    "    sentence_lengths2.append(sentence_length)\n",
    "for sentence in doc3.sents:\n",
    "    sentence_length = len(sentence)\n",
    "    sentence_lengths3.append(sentence_length)\n",
    "for sentence in doc4.sents:\n",
    "    sentence_length = len(sentence)\n",
    "    sentence_lengths4.append(sentence_length)\n",
    "for sentence in doc5.sents:\n",
    "    sentence_length = len(sentence)\n",
    "    sentence_lengths5.append(sentence_length)\n",
    "for sentence in doc6.sents:\n",
    "    sentence_length = len(sentence)\n",
    "    sentence_lengths6.append(sentence_length)\n",
    "    \n",
    "average_sentence_length1 = sum(sentence_lengths1) / len(sentence_lengths1)\n",
    "average_sentence_length2 = sum(sentence_lengths2) / len(sentence_lengths2)\n",
    "average_sentence_length3 = sum(sentence_lengths3) / len(sentence_lengths3)\n",
    "average_sentence_length4 = sum(sentence_lengths4) / len(sentence_lengths4)\n",
    "average_sentence_length5 = sum(sentence_lengths5) / len(sentence_lengths5)\n",
    "average_sentence_length6 = sum(sentence_lengths6) / len(sentence_lengths6)\n",
    "\n",
    "print(f'Средняя длина предложений в {name_book[0]}: {average_sentence_length1:.2f}')\n",
    "print(f'Средняя длина предложений в {name_book[1]}: {average_sentence_length2:.2f}')\n",
    "print(f'Средняя длина предложений в {name_book[2]}: {average_sentence_length3:.2f}')\n",
    "print(f'Средняя длина предложений в {name_book[3]}: {average_sentence_length4:.2f}')\n",
    "print(f'Средняя длина предложений в {name_book[4]}: {average_sentence_length5:.2f}')\n",
    "print(f'Средняя длина предложений в {name_book[5]}: {average_sentence_length6:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Вывод можно сделать такой: в начале своей карьеры Чехов пробовал себя, то есть писал и короткми предложениями и длинными, ему зашло писать длинными, в середине карьеры он писал длинными предложениями, но под конец решил всё-так писать короткими предложениями***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**дальше я не придумал что можно ещё сделать, если нужно будет что-то добавить, то скажите, я добавлю**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
